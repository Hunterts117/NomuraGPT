{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66afe12-f089-46d6-b155-550a12e0d4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdf2txt\n",
      "  Downloading pdf2txt-0.7.3-py3-none-any.whl (75 kB)\n",
      "                                              0.0/75.4 kB ? eta -:--:--\n",
      "     ----------------                         30.7/75.4 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------   71.7/75.4 kB 777.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 75.4/75.4 kB 592.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from pdf2txt) (2.0.1)\n",
      "Collecting pdf2image (from pdf2txt)\n",
      "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
      "Collecting pdfminer.six (from pdf2txt)\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "                                              0.0/5.6 MB ? eta -:--:--\n",
      "                                              0.1/5.6 MB 3.2 MB/s eta 0:00:02\n",
      "     -                                        0.2/5.6 MB 2.4 MB/s eta 0:00:03\n",
      "     --                                       0.3/5.6 MB 2.0 MB/s eta 0:00:03\n",
      "     ---                                      0.5/5.6 MB 2.8 MB/s eta 0:00:02\n",
      "     ------                                   1.0/5.6 MB 4.1 MB/s eta 0:00:02\n",
      "     ----------                               1.4/5.6 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------                          2.2/5.6 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------                          2.2/5.6 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------                       2.6/5.6 MB 6.2 MB/s eta 0:00:01\n",
      "     -----------------------                  3.3/5.6 MB 6.9 MB/s eta 0:00:01\n",
      "     -----------------------------            4.1/5.6 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------------------     5.2/5.6 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.6/5.6 MB 8.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from pdf2txt) (9.5.0)\n",
      "Collecting PyPDF2 (from pdf2txt)\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "                                              0.0/232.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 232.6/232.6 kB 13.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from pandas->pdf2txt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from pandas->pdf2txt) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from pandas->pdf2txt) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from pandas->pdf2txt) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from pdfminer.six->pdf2txt) (3.1.0)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->pdf2txt)\n",
      "  Downloading cryptography-40.0.2-cp36-abi3-win_amd64.whl (2.6 MB)\n",
      "                                              0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------                          1.0/2.6 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------             1.8/2.6 MB 19.7 MB/s eta 0:00:01\n",
      "     --------------------------------------   2.5/2.6 MB 17.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 15.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from cryptography>=36.0.0->pdfminer.six->pdf2txt) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->pdf2txt) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hunterfinn\\appdata\\roaming\\python\\python311\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pdf2txt) (2.21)\n",
      "Installing collected packages: PyPDF2, pdf2image, cryptography, pdfminer.six, pdf2txt\n",
      "Successfully installed PyPDF2-3.0.1 cryptography-40.0.2 pdf2image-1.16.3 pdf2txt-0.7.3 pdfminer.six-20221105\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec6395b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "\n",
    "# Setup your LLM\n",
    "\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    PromptHelper,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    StorageContext, \n",
    "    load_index_from_storage,\n",
    "    download_loader\n",
    ")\n",
    "from langchain.llms import OpenAI\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5cf41880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "\n",
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "\n",
    "#2 docx\n",
    "# DocxReader = download_loader(\"DocxReader\")\n",
    "# loader = DocxReader()\n",
    "# documents = loader.load_data(file=Path('./data/ac_open.docx'))\n",
    "\n",
    "# Create an index of your documents\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# save index to disk\n",
    "index.set_index_id(\"vector_index\")\n",
    "index.storage_context.persist('storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c726dc51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 22136 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "\n",
    "# Create an index of your documents by customized LLM \n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 512\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "# define LLM\n",
    "# llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\", max_tokens=num_output))\n",
    "# service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\", max_tokens=512))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "# build index\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# save index to disk\n",
    "index.set_index_id(\"vector_index\")\n",
    "index.storage_context.persist('storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "18731b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "> [get_response] Total LLM token usage: 2228 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "上週美國地區的重點新聞包括：美國總統川普宣布將暫停美國與墨西哥之間的貿易協定；美國聯邦最高法院裁定禁止美國政府對同性婚姻的歧視；美國聯邦最高法院裁定禁止美國政府對墨西哥移民的歧視；美國總統川普宣布將暫停美國與加拿大之\n"
     ]
    }
   ],
   "source": [
    "# Query your index!\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"上週美國地區的重點新聞?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f359b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "Loading indices with ids: ['vector_index']\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "> [retrieve] Total embedding token usage: 39 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "> [get_response] Total LLM token usage: 2147 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "野村環球時機多重資產基金的投資範圍包括國內外的證券投資，以及非投資等級的高風險債券。\n"
     ]
    }
   ],
   "source": [
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir='storage')\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "# Query your index!\n",
    "query_engine = index.as_query_engine()\n",
    "#response = query_engine.query(\"開戶申請需先準備甚麼資料?\")\n",
    "#response = query_engine.query(\"上週美國地區的重點新聞?\")\n",
    "response = query_engine.query(\"野村環球時機多重資產基金的投資範圍?\")\n",
    "#response = query_engine.query(\"野村環球時機多重資產基金的成立日期?\")\n",
    "#response = query_engine.query(\"野村環球時機多重資產基金的保管機構?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "111bd731-83c1-4e79-9c86-f811521d52ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading indices with ids: ['vector_index2']\n",
      "Loading indices with ids: ['vector_index2']\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 29 tokens\n",
      "> [retrieve] Total embedding token usage: 29 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1877 tokens\n",
      "> [get_response] Total LLM token usage: 1877 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "答案：\n",
      "\n",
      "開戶申請需先準備以下資料：\n",
      "\n",
      "(1)身分證正本正反面、第二證件正本之照片圖檔(如:護照、健保卡、駕照)\n",
      "(2)扣款帳戶資料（如晶片金融卡）\n",
      "(3)手寫簽名拍照圖檔\n",
      "(4)受益人身分證正反面影本及第二證件影本\n",
      "(5)約定帳戶存摺封面影本\n",
      "(6)未\n"
     ]
    }
   ],
   "source": [
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir='storage')\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "# Query your index!\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"開戶申請需先準備甚麼資料?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f4e5c",
   "metadata": {},
   "source": [
    "# Wikipedia Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4db9772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "wikidocs = loader.load_data(pages=['Cyclone Freddy'])\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Cyclone_Freddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "941c9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 4103 tokens\n"
     ]
    }
   ],
   "source": [
    "wiki_index = GPTSimpleVectorIndex(wikidocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e4a4dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3844 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cyclone Freddy is a very intense tropical cyclone that affected the Mascarene Islands, Madagascar, Mozambique, and Zimbabwe in February 2023. It is the longest-lived tropical cyclone on record, surpassing Hurricane John's record of 31 days. Freddy was once a powerful cyclone that was classified as a Category 5-equivalent tropical cyclone by the Joint Typhoon Warning Center (JTWC). It caused widespread damage and at least 29 deaths in Madagascar, Mozambique, and Zimbabwe. In Madagascar, over 14,000 homes were affected, with 5,500 destroyed, 3,079 flooded, and at least 9,696 damaged. At least 24,358 people were displaced, and nearly 25,000 customers were left without power at the height of the cyclone. In Saint-Paul, 20 tons of mangoes were destroyed, and Highway RD48 in Salazie was closed due to a landslide. Eleven mobile sites maintained by Orange S.A. were knocked offline in Tampon, Saint-Louis, and Saint-Paul.\n"
     ]
    }
   ],
   "source": [
    "response = wiki_index.query(\"What is cyclone freddy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2acc",
   "metadata": {},
   "source": [
    "# Customer Support Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "19f396a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./asos').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cb30944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 12584 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "946c44ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1317 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the United Arab Emirates, you have the option of signing up for ASOS Premier, which gives you free Standard and Express delivery all year round when you spend over 150 AED. It costs 200 AED and is valid on the order you purchase it on.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What premier service options do I have in the UAE?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77e646",
   "metadata": {},
   "source": [
    "# YouTube Video Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "89160742",
   "metadata": {},
   "outputs": [],
   "source": [
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=K7Kh9Ntd8VE&ab_channel=DaveNick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0995d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 18181 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "194e88fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4024 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Re-uploading other people's content without permission.\n",
      "2. Using copyrighted music.\n",
      "3. Not understanding how the YouTube algorithm works.\n",
      "4. Not researching the best niche for YouTube automation.\n",
      "5. Not optimizing the About section with relevant keywords.\n",
      "6. Not creating a logo and channel art that is professional and attractive.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What some YouTube automation mistakes to avoid?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a05da",
   "metadata": {},
   "source": [
    "# Chatbot Class - Just include your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a0e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24576df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(\"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\", index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {response['content']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
